{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "validategpu.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tcgb3ey5xHU",
        "colab_type": "code",
        "outputId": "e52ccb90-e42d-41e5-accc-e8b152075582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        }
      },
      "source": [
        "!pip install keras_vggface"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras_vggface\n",
            "  Downloading https://files.pythonhosted.org/packages/11/9c/d249cf4998344806d71b0351db690917413d1f7eaab83805f4095375e7a1/keras_vggface-0.5.tar.gz\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.16.3)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.3.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.8.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (4.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.13.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (2.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras_vggface) (3.13)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->keras_vggface) (0.46)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.7.1)\n",
            "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.13.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.33.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.0.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow->keras_vggface) (1.13.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->keras_vggface) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->keras_vggface) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow->keras_vggface) (41.0.1)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->keras_vggface) (3.0.5)\n",
            "Building wheels for collected packages: keras-vggface\n",
            "  Building wheel for keras-vggface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/aa/01/eb7baeb2f6e2d2f0d2aabddb5f01d57fa22fbd019ee2799bf5\n",
            "Successfully built keras-vggface\n",
            "Installing collected packages: keras-vggface\n",
            "Successfully installed keras-vggface-0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-CONgbi55tR",
        "colab_type": "code",
        "outputId": "2ec3f777-f6e3-45e1-8a4d-4cae2b51dbd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "!pip install keras==2.2.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/12/4cabc5c01451eb3b413d19ea151f36e33026fc0efb932bf51bcaf54acbf5/Keras-2.2.0-py2.py3-none-any.whl (300kB)\n",
            "\r\u001b[K     |█                               | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 6.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 30kB 8.8MB/s eta 0:00:01\r\u001b[K     |████▍                           | 40kB 5.6MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 61kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 71kB 9.2MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 81kB 10.2MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 92kB 11.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 102kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 112kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 122kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 133kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 143kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 153kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 163kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 174kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 184kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 194kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 204kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 215kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 225kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 235kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 245kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 256kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 266kB 9.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 276kB 9.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 286kB 9.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 296kB 9.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 307kB 9.1MB/s \n",
            "\u001b[?25hCollecting keras-preprocessing==1.0.1 (from keras==2.2.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/f8/33/275506afe1d96b221f66f95adba94d1b73f6b6087cfb6132a5655b6fe338/Keras_Preprocessing-1.0.1-py2.py3-none-any.whl\n",
            "Collecting keras-applications==1.0.2 (from keras==2.2.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/60/c557075e586e968d7a9c314aa38c236b37cb3ee6b37e8d57152b1a5e0b47/Keras_Applications-1.0.2-py2.py3-none-any.whl (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 25.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 35.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 41.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 46.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0) (1.16.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.0) (1.3.0)\n",
            "\u001b[31mERROR: tensorflow 1.13.1 has requirement keras-applications>=1.0.6, but you'll have keras-applications 1.0.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.13.1 has requirement keras-preprocessing>=1.0.5, but you'll have keras-preprocessing 1.0.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-preprocessing, keras-applications, keras\n",
            "  Found existing installation: Keras-Preprocessing 1.0.9\n",
            "    Uninstalling Keras-Preprocessing-1.0.9:\n",
            "      Successfully uninstalled Keras-Preprocessing-1.0.9\n",
            "  Found existing installation: Keras-Applications 1.0.7\n",
            "    Uninstalling Keras-Applications-1.0.7:\n",
            "      Successfully uninstalled Keras-Applications-1.0.7\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed keras-2.2.0 keras-applications-1.0.2 keras-preprocessing-1.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpozfvlbi0My",
        "colab_type": "code",
        "outputId": "5d7e770d-6cf8-45c2-c793-53365681f098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv3ezf8Wi5Ke",
        "colab_type": "code",
        "outputId": "785016f8-2d0a-4485-c6a1-177c55d72ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFKiRsykeQEW",
        "colab_type": "code",
        "outputId": "3573446d-50ba-4977-b50e-cd63cbf45403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6460
        }
      },
      "source": [
        "model = VGGFace(model='resnet50', include_top=False,input_shape=(224,224,3))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1/7x7_s2 (Conv2D)           (None, 112, 112, 64) 9408        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1/7x7_s2/bn (BatchNormaliza (None, 112, 112, 64) 256         conv1/7x7_s2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 112, 112, 64) 0           conv1/7x7_s2/bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 55, 55, 64)   0           activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_reduce (Conv2D)     (None, 55, 55, 64)   4096        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 55, 55, 64)   0           conv2_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_51[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 55, 55, 64)   0           conv2_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_proj (Conv2D)       (None, 55, 55, 256)  16384       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv2_1_1x1_proj/bn (BatchNorma (None, 55, 55, 256)  1024        conv2_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 55, 55, 256)  0           conv2_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv2_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 55, 55, 256)  0           add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 55, 55, 64)   0           conv2_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_54[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 55, 55, 64)   0           conv2_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_2_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 55, 55, 256)  0           conv2_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 55, 55, 256)  0           add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_reduce (Conv2D)     (None, 55, 55, 64)   16384       activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_reduce/bn (BatchNor (None, 55, 55, 64)   256         conv2_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 55, 55, 64)   0           conv2_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_3x3 (Conv2D)            (None, 55, 55, 64)   36864       activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_3x3/bn (BatchNormalizat (None, 55, 55, 64)   256         conv2_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 55, 55, 64)   0           conv2_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_increase (Conv2D)   (None, 55, 55, 256)  16384       activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2_3_1x1_increase/bn (BatchN (None, 55, 55, 256)  1024        conv2_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 55, 55, 256)  0           conv2_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 55, 55, 256)  0           add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_reduce (Conv2D)     (None, 28, 28, 128)  32768       activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 28, 28, 128)  0           conv3_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 28, 28, 128)  0           conv3_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_61[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_proj (Conv2D)       (None, 28, 28, 512)  131072      activation_59[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv3_1_1x1_proj/bn (BatchNorma (None, 28, 28, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 28, 28, 512)  0           conv3_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv3_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 28, 28, 512)  0           add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 28, 28, 128)  0           conv3_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 28, 28, 128)  0           conv3_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_64[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_2_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 28, 28, 512)  0           conv3_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 28, 28, 512)  0           add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 28, 28, 128)  0           conv3_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 28, 28, 128)  0           conv3_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_3_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 28, 28, 512)  0           conv3_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 28, 28, 512)  0           add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_reduce (Conv2D)     (None, 28, 28, 128)  65536       activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_reduce/bn (BatchNor (None, 28, 28, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 28, 28, 128)  0           conv3_4_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_3x3 (Conv2D)            (None, 28, 28, 128)  147456      activation_69[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_3x3/bn (BatchNormalizat (None, 28, 28, 128)  512         conv3_4_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 28, 28, 128)  0           conv3_4_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_increase (Conv2D)   (None, 28, 28, 512)  65536       activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv3_4_1x1_increase/bn (BatchN (None, 28, 28, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 28, 28, 512)  0           conv3_4_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 28, 28, 512)  0           add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_reduce (Conv2D)     (None, 14, 14, 256)  131072      activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 14, 14, 256)  0           conv4_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_72[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 14, 14, 256)  0           conv4_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_proj (Conv2D)       (None, 14, 14, 1024) 524288      activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv4_1_1x1_proj/bn (BatchNorma (None, 14, 14, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 14, 14, 1024) 0           conv4_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv4_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 14, 14, 1024) 0           add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 14, 14, 256)  0           conv4_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 14, 14, 256)  0           conv4_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_76[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_2_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 14, 14, 1024) 0           conv4_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 14, 14, 1024) 0           add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 14, 14, 256)  0           conv4_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 14, 14, 256)  0           conv4_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_79[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_3_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 14, 14, 1024) 0           conv4_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_77[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 14, 14, 1024) 0           add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 14, 14, 256)  0           conv4_4_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_4_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 14, 14, 256)  0           conv4_4_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_4_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 14, 14, 1024) 0           conv4_4_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 14, 14, 1024) 0           add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 14, 14, 256)  0           conv4_5_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_5_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 14, 14, 256)  0           conv4_5_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_5_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 14, 14, 1024) 0           conv4_5_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_83[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 14, 14, 1024) 0           add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_reduce (Conv2D)     (None, 14, 14, 256)  262144      activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_reduce/bn (BatchNor (None, 14, 14, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 14, 14, 256)  0           conv4_6_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_3x3 (Conv2D)            (None, 14, 14, 256)  589824      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_3x3/bn (BatchNormalizat (None, 14, 14, 256)  1024        conv4_6_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 14, 14, 256)  0           conv4_6_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_increase (Conv2D)   (None, 14, 14, 1024) 262144      activation_88[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv4_6_1x1_increase/bn (BatchN (None, 14, 14, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 14, 14, 1024) 0           conv4_6_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_86[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 14, 14, 1024) 0           add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_reduce (Conv2D)     (None, 7, 7, 512)    524288      activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_1_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 7, 7, 512)    0           conv5_1_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_1_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 7, 7, 512)    0           conv5_1_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_proj (Conv2D)       (None, 7, 7, 2048)   2097152     activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_1_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "conv5_1_1x1_proj/bn (BatchNorma (None, 7, 7, 2048)   8192        conv5_1_1x1_proj[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 7, 7, 2048)   0           conv5_1_1x1_increase/bn[0][0]    \n",
            "                                                                 conv5_1_1x1_proj/bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 7, 7, 2048)   0           add_30[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_2_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 7, 7, 512)    0           conv5_2_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_2_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 7, 7, 512)    0           conv5_2_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_2_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_2_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_31 (Add)                    (None, 7, 7, 2048)   0           conv5_2_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_95 (Activation)      (None, 7, 7, 2048)   0           add_31[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_reduce (Conv2D)     (None, 7, 7, 512)    1048576     activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_reduce/bn (BatchNor (None, 7, 7, 512)    2048        conv5_3_1x1_reduce[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "activation_96 (Activation)      (None, 7, 7, 512)    0           conv5_3_1x1_reduce/bn[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_3x3 (Conv2D)            (None, 7, 7, 512)    2359296     activation_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_3x3/bn (BatchNormalizat (None, 7, 7, 512)    2048        conv5_3_3x3[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_97 (Activation)      (None, 7, 7, 512)    0           conv5_3_3x3/bn[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_increase (Conv2D)   (None, 7, 7, 2048)   1048576     activation_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv5_3_1x1_increase/bn (BatchN (None, 7, 7, 2048)   8192        conv5_3_1x1_increase[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "add_32 (Add)                    (None, 7, 7, 2048)   0           conv5_3_1x1_increase/bn[0][0]    \n",
            "                                                                 activation_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_98 (Activation)      (None, 7, 7, 2048)   0           add_32[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "avg_pool (AveragePooling2D)     (None, 1, 1, 2048)   0           activation_98[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 23,561,152\n",
            "Trainable params: 23,508,032\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LCgb6H7KazL",
        "colab_type": "code",
        "outputId": "26f0d642-610e-4d9e-8cda-6a5908ebaca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "import numpy as np\n",
        "from keras import losses\n",
        "from keras import backend as K\n",
        "\n",
        "def baseline_model():\n",
        "    x = Input(shape=(1,1,2048))\n",
        "\n",
        "    # x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n",
        "    # x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n",
        "    #\n",
        "    # x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n",
        "    # x_dot = Flatten()(x_dot)\n",
        "\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x), GlobalAvgPool2D()(x)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x), GlobalAvgPool2D()(x)])\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x = Multiply()([x1, x2])\n",
        "\n",
        "    x = Concatenate(axis=-1)([x, x3])\n",
        "\n",
        "    x = Dense(100, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "    \n",
        "layer1 = Input(shape=(1,1,2048))\n",
        "\n",
        "layer2 = GlobalMaxPool2D()(layer1)\n",
        "\n",
        "model = Model(inputs=layer1,outputs=layer2)\n",
        "\n",
        "inp = np.random.rand(1,1,1,2048)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_28 (InputLayer)        (None, 1, 1, 2048)        0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_32 (Glo (None, 2048)              0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[0.381931   0.0566115  0.05444313 ... 0.18831639 0.4700506  0.774913  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUJBNO9BzWT6",
        "colab_type": "code",
        "outputId": "d040b8ff-092c-4774-893f-cf90b7382a1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.predict(inp).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2048)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxl0rfcwjE0P",
        "colab_type": "code",
        "outputId": "dbbf2a21-1bcf-44c8-ad5d-081fc1d5c0c6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-463f2fb7-ab6b-4755-bd3d-c32553ac6fce\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-463f2fb7-ab6b-4755-bd3d-c32553ac6fce\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving train.zip to train.zip\n",
            "User uploaded file \"train.zip\" with length 71899790 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0ov69GuEjsF",
        "colab_type": "code",
        "outputId": "af4f37db-4108-481e-91e3-862a09d00a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/  submission.csv  test.zip  train_relationships.csv  train.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UWdp_Vbq3y-",
        "colab_type": "code",
        "outputId": "a2a0f0a8-17b0-4c5c-aa6c-6226e965a659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# importing required modules \n",
        "from zipfile import ZipFile \n",
        "\n",
        "# specifying the zip file name \n",
        "file_name = \"../content/train.zip\"\n",
        "\n",
        "# opening the zip file in READ mode \n",
        "with ZipFile(file_name, 'r') as zipm: \n",
        "\t# printing all the contents of the zip file \n",
        "  #zipm.printdir() \n",
        "\n",
        "\t# extracting all the files \n",
        "\tprint('Extracting all the files now...') \n",
        "\tzipm.extractall(\"../content/train\") \n",
        "\tprint('Done!') \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting all the files now...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1uV8Os3soW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Cj-6uxptvOL",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lollB7YJtxoT",
        "colab_type": "code",
        "outputId": "af9f448f-5003-44bd-aeb2-3710863af327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/    \u001b[01;34mtest\u001b[0m/     \u001b[01;34mtrain\u001b[0m/                   train.zip\n",
            "submission.csv  test.zip  train_relationships.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPXr0rktvVm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "from glob import glob\n",
        "from random import choice, sample\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9_YPTYqwdaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_file_path = \"../content/train_relationships.csv\"\n",
        "train_folders_path = \"../content/train/\"\n",
        "val_famillies = \"F08\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9M0YyLBAwt5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_images = glob(train_folders_path + \"*/*/*.jpg\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55Eb-R69w7uT",
        "colab_type": "code",
        "outputId": "3045b500-6184-4ea2-e0de-2a9571c19541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12379"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHJbTze8w9sf",
        "colab_type": "code",
        "outputId": "64ccf7cf-36a1-4022-c648-c1bcfd7a4009",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "all_images[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'../content/train/F0874/MID3/P09232_face2.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qFf4qc1xAzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = [x for x in all_images if val_famillies not in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdeyWb6RxODd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_images = [x for x in all_images if val_famillies in x]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4QoruyKxWg9",
        "colab_type": "code",
        "outputId": "eea601fe-49c0-4051-9958-2e24fb618c16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(val_images)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1328"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5f0I3EHVxa2s",
        "colab_type": "code",
        "outputId": "bb1641da-6126-4e99-d0b4-a44434ee42b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "val_images[-5:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['../content/train/F0846/MID2/P08933_face2.jpg',\n",
              " '../content/train/F0846/MID4/P08937_face2.jpg',\n",
              " '../content/train/F0846/MID4/P08942_face1.jpg',\n",
              " '../content/train/F0846/MID4/P08941_face1.jpg',\n",
              " '../content/train/F0846/MID4/P08936_face1.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htv0ILGDxkhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_person_to_images_map = defaultdict(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjULXzzyyMj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ppl = [x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2] for x in all_images]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYVysMEFyR8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in train_images:\n",
        "    train_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NiK46YzyUHF",
        "colab_type": "code",
        "outputId": "ccc6b73d-6d73-4163-d007-7acdea825ba1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_person_to_images_map.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['F0058/MID3', 'F0058/MID5', 'F0058/MID1', 'F0058/MID6', 'F0058/MID4', 'F0113/MID3', 'F0113/MID5', 'F0113/MID1', 'F0113/MID2', 'F0113/MID4', 'F0771/MID3', 'F0771/MID5', 'F0771/MID1', 'F0771/MID6', 'F0771/MID2', 'F0771/MID4', 'F0675/MID3', 'F0675/MID5', 'F0675/MID1', 'F0675/MID6', 'F0675/MID2', 'F0675/MID4', 'F0589/MID3', 'F0589/MID2', 'F0589/MID4', 'F0909/MID3', 'F0909/MID1', 'F0909/MID2', 'F0909/MID4', 'F0233/MID3', 'F0233/MID5', 'F0233/MID1', 'F0233/MID6', 'F0233/MID2', 'F0683/MID3', 'F0683/MID5', 'F0683/MID1', 'F0683/MID6', 'F0683/MID2', 'F0683/MID4', 'F0161/MID3', 'F0161/MID1', 'F0161/MID2', 'F0593/MID3', 'F0593/MID5', 'F0593/MID1', 'F0593/MID2', 'F0593/MID4', 'F0531/MID3', 'F0531/MID1', 'F0531/MID2', 'F0531/MID4', 'F0630/MID3', 'F0630/MID1', 'F0630/MID2', 'F0440/MID3', 'F0440/MID1', 'F0440/MID2', 'F0440/MID4', 'F0693/MID3', 'F0693/MID1', 'F0693/MID2', 'F0693/MID4', 'F0408/MID3', 'F0408/MID1', 'F0408/MID2', 'F0408/MID4', 'F0579/MID3', 'F0579/MID5', 'F0579/MID1', 'F0579/MID8', 'F0579/MID6', 'F0579/MID7', 'F0579/MID9', 'F0579/MID2', 'F0579/MID4', 'F0730/MID3', 'F0730/MID5', 'F0730/MID1', 'F0730/MID6', 'F0730/MID2', 'F0730/MID4', 'F0565/MID3', 'F0565/MID1', 'F0565/MID2', 'F0565/MID4', 'F0099/MID10', 'F0099/MID3', 'F0099/MID5', 'F0099/MID1', 'F0099/MID8', 'F0099/MID6', 'F0099/MID7', 'F0099/MID9', 'F0099/MID2', 'F0099/MID4', 'F0736/MID3', 'F0736/MID5', 'F0736/MID1', 'F0736/MID8', 'F0736/MID6', 'F0736/MID7', 'F0736/MID9', 'F0736/MID2', 'F0736/MID4', 'F0498/MID1', 'F0498/MID6', 'F0498/MID4', 'F0106/MID3', 'F0106/MID5', 'F0106/MID1', 'F0106/MID6', 'F0106/MID7', 'F0106/MID2', 'F0106/MID4', 'F0488/MID3', 'F0488/MID1', 'F0488/MID2', 'F0488/MID4', 'F0199/MID3', 'F0199/MID5', 'F0199/MID1', 'F0199/MID6', 'F0199/MID2', 'F0199/MID4', 'F0470/MID3', 'F0470/MID2', 'F0470/MID4', 'F0945/MID3', 'F0945/MID1', 'F0945/MID2', 'F0794/MID3', 'F0794/MID5', 'F0794/MID1', 'F0794/MID6', 'F0794/MID7', 'F0794/MID2', 'F0794/MID4', 'F0196/MID3', 'F0196/MID5', 'F0196/MID1', 'F0196/MID2', 'F0282/MID10', 'F0282/MID3', 'F0282/MID5', 'F0282/MID1', 'F0282/MID8', 'F0282/MID6', 'F0282/MID7', 'F0282/MID9', 'F0282/MID2', 'F0282/MID4', 'F0215/MID3', 'F0215/MID5', 'F0215/MID1', 'F0215/MID8', 'F0215/MID6', 'F0215/MID9', 'F0215/MID2', 'F0432/MID3', 'F0432/MID5', 'F0432/MID1', 'F0432/MID6', 'F0432/MID2', 'F0432/MID4', 'F0900/MID3', 'F0900/MID1', 'F0900/MID2', 'F0439/MID3', 'F0439/MID5', 'F0439/MID1', 'F0439/MID6', 'F0439/MID7', 'F0439/MID9', 'F0439/MID2', 'F0439/MID4', 'F0158/MID3', 'F0158/MID5', 'F0158/MID1', 'F0158/MID8', 'F0158/MID6', 'F0158/MID7', 'F0158/MID2', 'F0158/MID4', 'F0978/MID3', 'F0978/MID5', 'F0978/MID1', 'F0978/MID8', 'F0978/MID6', 'F0978/MID7', 'F0978/MID2', 'F0978/MID4', 'F0501/MID3', 'F0501/MID5', 'F0501/MID1', 'F0501/MID2', 'F0501/MID4', 'F0689/MID3', 'F0689/MID1', 'F0689/MID2', 'F0689/MID4', 'F0024/MID3', 'F0024/MID5', 'F0024/MID1', 'F0024/MID2', 'F0024/MID4', 'F0486/MID3', 'F0486/MID5', 'F0486/MID1', 'F0486/MID6', 'F0486/MID2', 'F0486/MID4', 'F0686/MID3', 'F0686/MID5', 'F0686/MID1', 'F0686/MID6', 'F0686/MID2', 'F0538/MID3', 'F0538/MID5', 'F0538/MID1', 'F0538/MID2', 'F0538/MID4', 'F0942/MID3', 'F0942/MID1', 'F0942/MID2', 'F0942/MID4', 'F0134/MID1', 'F0134/MID2', 'F0134/MID4', 'F0376/MID10', 'F0376/MID3', 'F0376/MID5', 'F0376/MID1', 'F0376/MID8', 'F0376/MID6', 'F0376/MID7', 'F0376/MID9', 'F0376/MID2', 'F0376/MID4', 'F0416/MID3', 'F0416/MID5', 'F0416/MID1', 'F0416/MID6', 'F0515/MID3', 'F0515/MID1', 'F0515/MID2', 'F0515/MID4', 'F0530/MID3', 'F0530/MID1', 'F0530/MID2', 'F0530/MID4', 'F0988/MID3', 'F0988/MID5', 'F0988/MID1', 'F0988/MID8', 'F0988/MID6', 'F0988/MID7', 'F0988/MID9', 'F0988/MID2', 'F0988/MID4', 'F0203/MID10', 'F0203/MID3', 'F0203/MID5', 'F0203/MID1', 'F0203/MID8', 'F0203/MID6', 'F0203/MID7', 'F0203/MID2', 'F0203/MID4', 'F0582/MID3', 'F0582/MID1', 'F0582/MID2', 'F0165/MID3', 'F0165/MID5', 'F0165/MID1', 'F0165/MID8', 'F0165/MID6', 'F0165/MID7', 'F0165/MID9', 'F0165/MID2', 'F0165/MID4', 'F0998/MID3', 'F0998/MID5', 'F0998/MID1', 'F0998/MID6', 'F0998/MID2', 'F0998/MID4', 'F0302/MID3', 'F0302/MID1', 'F0302/MID2', 'F0302/MID4', 'F0769/MID3', 'F0769/MID5', 'F0769/MID1', 'F0769/MID6', 'F0769/MID7', 'F0769/MID2', 'F0769/MID4', 'F0253/MID3', 'F0253/MID2', 'F0197/MID3', 'F0197/MID5', 'F0197/MID1', 'F0197/MID6', 'F0197/MID2', 'F0197/MID4', 'F0051/MID3', 'F0051/MID1', 'F0051/MID6', 'F0051/MID2', 'F0051/MID4', 'F0655/MID3', 'F0655/MID1', 'F0655/MID2', 'F0517/MID3', 'F0517/MID5', 'F0517/MID1', 'F0517/MID2', 'F0517/MID4', 'F0373/MID3', 'F0373/MID1', 'F0373/MID2', 'F0373/MID4', 'F0916/MID3', 'F0916/MID1', 'F0916/MID2', 'F0214/MID3', 'F0214/MID1', 'F0214/MID2', 'F0214/MID4', 'F0097/MID3', 'F0097/MID5', 'F0097/MID2', 'F0097/MID4', 'F0959/MID3', 'F0959/MID5', 'F0959/MID1', 'F0959/MID2', 'F0959/MID4', 'F0711/MID3', 'F0711/MID1', 'F0711/MID2', 'F0711/MID4', 'F0752/MID10', 'F0752/MID3', 'F0752/MID5', 'F0752/MID1', 'F0752/MID8', 'F0752/MID6', 'F0752/MID7', 'F0752/MID9', 'F0752/MID2', 'F0752/MID4', 'F0967/MID3', 'F0967/MID1', 'F0967/MID2', 'F0967/MID4', 'F0973/MID3', 'F0973/MID1', 'F0973/MID2', 'F0780/MID3', 'F0780/MID1', 'F0780/MID2', 'F0468/MID3', 'F0468/MID1', 'F0468/MID2', 'F0468/MID4', 'F0077/MID3', 'F0077/MID1', 'F0077/MID6', 'F0077/MID2', 'F0077/MID4', 'F0966/MID3', 'F0966/MID1', 'F0966/MID2', 'F0966/MID4', 'F0238/MID3', 'F0238/MID5', 'F0238/MID1', 'F0238/MID6', 'F0238/MID4', 'F0169/MID3', 'F0169/MID1', 'F0169/MID2', 'F0169/MID4', 'F0206/MID10', 'F0206/MID3', 'F0206/MID1', 'F0206/MID8', 'F0206/MID7', 'F0206/MID11', 'F0206/MID9', 'F0206/MID2', 'F0206/MID13', 'F0206/MID4', 'F0206/MID12', 'F0993/MID3', 'F0993/MID5', 'F0993/MID6', 'F0993/MID2', 'F0993/MID4', 'F0487/MID3', 'F0487/MID1', 'F0487/MID2', 'F0679/MID1', 'F0679/MID2', 'F0745/MID3', 'F0745/MID1', 'F0745/MID2', 'F0745/MID4', 'F0038/MID3', 'F0038/MID5', 'F0038/MID1', 'F0038/MID6', 'F0038/MID2', 'F0038/MID4', 'F0059/MID3', 'F0059/MID1', 'F0059/MID2', 'F0059/MID4', 'F0623/MID1', 'F0623/MID2', 'F0718/MID3', 'F0718/MID1', 'F0718/MID2', 'F0718/MID4', 'F0473/MID3', 'F0473/MID5', 'F0473/MID6', 'F0473/MID2', 'F0473/MID4', 'F0791/MID3', 'F0791/MID5', 'F0791/MID1', 'F0791/MID8', 'F0791/MID6', 'F0791/MID2', 'F0791/MID4', 'F0631/MID3', 'F0631/MID5', 'F0631/MID1', 'F0631/MID2', 'F0631/MID4', 'F0112/MID3', 'F0112/MID1', 'F0112/MID2', 'F0112/MID4', 'F0040/MID3', 'F0040/MID1', 'F0688/MID3', 'F0688/MID1', 'F0688/MID2', 'F0688/MID4', 'F0049/MID3', 'F0049/MID5', 'F0049/MID6', 'F0049/MID7', 'F0049/MID4', 'F0965/MID3', 'F0965/MID5', 'F0965/MID1', 'F0965/MID8', 'F0965/MID6', 'F0965/MID7', 'F0965/MID4', 'F0590/MID3', 'F0590/MID5', 'F0590/MID1', 'F0590/MID8', 'F0590/MID6', 'F0590/MID7', 'F0590/MID2', 'F0590/MID4', 'F0638/MID3', 'F0638/MID5', 'F0638/MID1', 'F0638/MID6', 'F0638/MID7', 'F0638/MID2', 'F0638/MID4', 'F0119/MID3', 'F0119/MID1', 'F0119/MID2', 'F0656/MID5', 'F0656/MID1', 'F0656/MID2', 'F0656/MID4', 'F0983/MID3', 'F0983/MID1', 'F0983/MID2', 'F0983/MID4', 'F0691/MID1', 'F0691/MID2', 'F0981/MID3', 'F0981/MID5', 'F0981/MID1', 'F0981/MID2', 'F0981/MID4', 'F0297/MID3', 'F0297/MID1', 'F0297/MID2', 'F0297/MID4', 'F0054/MID3', 'F0054/MID1', 'F0054/MID2', 'F0054/MID4', 'F0290/MID3', 'F0290/MID5', 'F0290/MID1', 'F0290/MID6', 'F0290/MID2', 'F0290/MID4', 'F0512/MID3', 'F0512/MID5', 'F0512/MID1', 'F0512/MID4', 'F0537/MID3', 'F0537/MID5', 'F0537/MID1', 'F0537/MID6', 'F0537/MID2', 'F0537/MID4', 'F0502/MID1', 'F0502/MID2', 'F0744/MID1', 'F0744/MID2', 'F0737/MID3', 'F0737/MID5', 'F0737/MID1', 'F0737/MID2', 'F0737/MID4', 'F0390/MID10', 'F0390/MID3', 'F0390/MID5', 'F0390/MID1', 'F0390/MID8', 'F0390/MID6', 'F0390/MID7', 'F0390/MID11', 'F0390/MID9', 'F0390/MID2', 'F0390/MID4', 'F0690/MID10', 'F0690/MID3', 'F0690/MID5', 'F0690/MID1', 'F0690/MID8', 'F0690/MID6', 'F0690/MID7', 'F0690/MID9', 'F0690/MID2', 'F0690/MID4', 'F0499/MID3', 'F0499/MID1', 'F0499/MID2', 'F0610/MID3', 'F0610/MID5', 'F0610/MID1', 'F0610/MID6', 'F0610/MID7', 'F0610/MID2', 'F0610/MID4', 'F0202/MID3', 'F0202/MID5', 'F0202/MID1', 'F0202/MID2', 'F0202/MID4', 'F0063/MID3', 'F0063/MID1', 'F0452/MID1', 'F0452/MID2', 'F0672/MID3', 'F0672/MID5', 'F0672/MID1', 'F0672/MID6', 'F0672/MID7', 'F0672/MID2', 'F0672/MID4', 'F0368/MID3', 'F0368/MID5', 'F0368/MID1', 'F0368/MID6', 'F0368/MID7', 'F0368/MID2', 'F0368/MID4', 'F0164/MID3', 'F0164/MID1', 'F0164/MID2', 'F0164/MID4', 'F0332/MID3', 'F0332/MID5', 'F0332/MID1', 'F0332/MID6', 'F0332/MID2', 'F0332/MID4', 'F0524/MID3', 'F0524/MID1', 'F0639/MID3', 'F0639/MID5', 'F0639/MID1', 'F0639/MID6', 'F0639/MID7', 'F0639/MID2', 'F0639/MID4', 'F0361/MID3', 'F0361/MID1', 'F0361/MID2', 'F0361/MID4', 'F0494/MID3', 'F0494/MID5', 'F0494/MID1', 'F0494/MID2', 'F0494/MID4', 'F0670/MID1', 'F0670/MID2', 'F0601/MID10', 'F0601/MID41', 'F0601/MID23', 'F0601/MID20', 'F0601/MID18', 'F0601/MID3', 'F0601/MID40', 'F0601/MID28', 'F0601/MID26', 'F0601/MID39', 'F0601/MID32', 'F0601/MID36', 'F0601/MID22', 'F0601/MID37', 'F0601/MID42', 'F0601/MID14', 'F0601/MID16', 'F0601/MID27', 'F0601/MID25', 'F0601/MID5', 'F0601/MID1', 'F0601/MID8', 'F0601/MID6', 'F0601/MID31', 'F0601/MID17', 'F0601/MID30', 'F0601/MID7', 'F0601/MID19', 'F0601/MID15', 'F0601/MID34', 'F0601/MID11', 'F0601/MID9', 'F0601/MID2', 'F0601/MID35', 'F0601/MID33', 'F0601/MID21', 'F0601/MID13', 'F0601/MID4', 'F0601/MID29', 'F0601/MID24', 'F0601/MID12', 'F0091/MID3', 'F0091/MID1', 'F0091/MID8', 'F0091/MID6', 'F0091/MID7', 'F0091/MID2', 'F0091/MID4', 'F0568/MID3', 'F0568/MID5', 'F0568/MID1', 'F0568/MID2', 'F0568/MID4', 'F0130/MID3', 'F0130/MID5', 'F0130/MID1', 'F0130/MID4', 'F0249/MID10', 'F0249/MID3', 'F0249/MID5', 'F0249/MID1', 'F0249/MID8', 'F0249/MID6', 'F0249/MID7', 'F0249/MID9', 'F0249/MID2', 'F0249/MID4', 'F0703/MID10', 'F0703/MID3', 'F0703/MID5', 'F0703/MID1', 'F0703/MID6', 'F0703/MID7', 'F0703/MID9', 'F0703/MID2', 'F0703/MID4', 'F0765/MID3', 'F0765/MID5', 'F0765/MID1', 'F0765/MID2', 'F0765/MID4', 'F0664/MID3', 'F0664/MID5', 'F0664/MID1', 'F0664/MID8', 'F0664/MID6', 'F0664/MID7', 'F0664/MID9', 'F0664/MID2', 'F0664/MID4', 'F0784/MID3', 'F0784/MID1', 'F0784/MID2', 'F0677/MID1', 'F0677/MID2', 'F0704/MID3', 'F0704/MID5', 'F0704/MID2', 'F0704/MID4', 'F0064/MID3', 'F0064/MID5', 'F0064/MID1', 'F0064/MID6', 'F0064/MID2', 'F0064/MID4', 'F0385/MID3', 'F0385/MID1', 'F0385/MID4', 'F0070/MID5', 'F0070/MID1', 'F0070/MID7', 'F0070/MID2', 'F0070/MID4', 'F0016/MID3', 'F0016/MID5', 'F0016/MID1', 'F0016/MID2', 'F0016/MID4', 'F0284/MID3', 'F0284/MID5', 'F0284/MID1', 'F0284/MID6', 'F0284/MID2', 'F0284/MID4', 'F0159/MID3', 'F0159/MID1', 'F0159/MID2', 'F0183/MID3', 'F0183/MID5', 'F0183/MID1', 'F0183/MID2', 'F0183/MID4', 'F0167/MID3', 'F0167/MID5', 'F0167/MID1', 'F0167/MID8', 'F0167/MID6', 'F0167/MID7', 'F0167/MID2', 'F0167/MID4', 'F0443/MID3', 'F0443/MID5', 'F0443/MID1', 'F0443/MID2', 'F0443/MID4', 'F0508/MID3', 'F0508/MID5', 'F0508/MID1', 'F0508/MID2', 'F0508/MID4', 'F0438/MID3', 'F0438/MID5', 'F0438/MID1', 'F0438/MID6', 'F0438/MID7', 'F0438/MID2', 'F0607/MID3', 'F0607/MID1', 'F0607/MID2', 'F0561/MID3', 'F0561/MID5', 'F0561/MID1', 'F0561/MID4', 'F0592/MID3', 'F0592/MID1', 'F0592/MID2', 'F0592/MID4', 'F0154/MID3', 'F0154/MID1', 'F0154/MID2', 'F0154/MID4', 'F0313/MID1', 'F0313/MID2', 'F0264/MID10', 'F0264/MID1', 'F0264/MID8', 'F0264/MID6', 'F0264/MID7', 'F0264/MID9', 'F0264/MID2', 'F0264/MID4', 'F0783/MID3', 'F0783/MID5', 'F0783/MID1', 'F0783/MID2', 'F0783/MID4', 'F0039/MID1', 'F0039/MID4', 'F0081/MID1', 'F0081/MID8', 'F0081/MID6', 'F0081/MID7', 'F0081/MID2', 'F0081/MID4', 'F0584/MID3', 'F0584/MID1', 'F0584/MID2', 'F0584/MID4', 'F0641/MID3', 'F0641/MID5', 'F0641/MID1', 'F0641/MID6', 'F0641/MID7', 'F0641/MID2', 'F0641/MID4', 'F0170/MID3', 'F0170/MID1', 'F0170/MID2', 'F0170/MID4', 'F0787/MID3', 'F0787/MID1', 'F0787/MID2', 'F0787/MID4', 'F0774/MID3', 'F0774/MID5', 'F0774/MID1', 'F0774/MID6', 'F0774/MID7', 'F0774/MID2', 'F0774/MID4', 'F0984/MID3', 'F0984/MID5', 'F0984/MID1', 'F0984/MID6', 'F0984/MID7', 'F0984/MID2', 'F0984/MID4', 'F0674/MID3', 'F0674/MID5', 'F0674/MID1', 'F0674/MID2', 'F0674/MID4', 'F0696/MID3', 'F0696/MID5', 'F0696/MID1', 'F0696/MID6', 'F0696/MID7', 'F0696/MID2', 'F0696/MID4', 'F0413/MID3', 'F0413/MID5', 'F0413/MID1', 'F0413/MID6', 'F0413/MID2', 'F0299/MID3', 'F0299/MID5', 'F0299/MID1', 'F0299/MID2', 'F0162/MID3', 'F0162/MID5', 'F0162/MID1', 'F0162/MID2', 'F0162/MID4', 'F0971/MID3', 'F0971/MID5', 'F0971/MID1', 'F0971/MID2', 'F0604/MID3', 'F0604/MID1', 'F0604/MID2', 'F0604/MID4', 'F0122/MID3', 'F0122/MID5', 'F0122/MID1', 'F0122/MID7', 'F0122/MID2', 'F0122/MID4', 'F0357/MID10', 'F0357/MID3', 'F0357/MID5', 'F0357/MID1', 'F0357/MID8', 'F0357/MID6', 'F0357/MID7', 'F0357/MID11', 'F0357/MID9', 'F0357/MID2', 'F0357/MID13', 'F0357/MID4', 'F0357/MID12', 'F0642/MID3', 'F0642/MID5', 'F0642/MID1', 'F0642/MID6', 'F0642/MID7', 'F0642/MID2', 'F0642/MID4', 'F0010/MID3', 'F0010/MID1', 'F0010/MID2', 'F0010/MID4', 'F0367/MID3', 'F0367/MID1', 'F0367/MID2', 'F0367/MID4', 'F0758/MID1', 'F0758/MID2', 'F0661/MID3', 'F0661/MID1', 'F0661/MID2', 'F0661/MID4', 'F0393/MID5', 'F0393/MID1', 'F0393/MID2', 'F0393/MID4', 'F0789/MID1', 'F0789/MID2', 'F0023/MID3', 'F0023/MID5', 'F0023/MID1', 'F0023/MID6', 'F0023/MID2', 'F0023/MID4', 'F0228/MID1', 'F0228/MID2', 'F0228/MID4', 'F0191/MID3', 'F0191/MID1', 'F0191/MID2', 'F0191/MID4', 'F0369/MID3', 'F0369/MID1', 'F0369/MID2', 'F0188/MID3', 'F0188/MID5', 'F0188/MID1', 'F0188/MID6', 'F0188/MID2', 'F0188/MID4', 'F0399/MID3', 'F0399/MID1', 'F0399/MID2', 'F0635/MID3', 'F0635/MID5', 'F0635/MID1', 'F0635/MID2', 'F0635/MID4', 'F0110/MID3', 'F0110/MID1', 'F0110/MID2', 'F0110/MID4', 'F0705/MID3', 'F0705/MID1', 'F0705/MID2', 'F0705/MID4', 'F0344/MID5', 'F0344/MID1', 'F0344/MID2', 'F0344/MID4', 'F0050/MID3', 'F0050/MID1', 'F0050/MID2', 'F0669/MID3', 'F0669/MID1', 'F0669/MID2', 'F0669/MID4', 'F0410/MID3', 'F0410/MID1', 'F0489/MID1', 'F0489/MID2', 'F0489/MID4', 'F0762/MID3', 'F0762/MID5', 'F0762/MID1', 'F0762/MID6', 'F0762/MID2', 'F0762/MID4', 'F0109/MID3', 'F0109/MID1', 'F0109/MID2', 'F0109/MID4', 'F0403/MID3', 'F0403/MID5', 'F0403/MID1', 'F0403/MID2', 'F0403/MID4', 'F0678/MID5', 'F0678/MID1', 'F0678/MID2', 'F0678/MID4', 'F0277/MID3', 'F0277/MID1', 'F0277/MID2', 'F0277/MID4', 'F0124/MID3', 'F0124/MID1', 'F0124/MID2', 'F0901/MID3', 'F0901/MID5', 'F0901/MID1', 'F0901/MID2', 'F0901/MID4', 'F0424/MID3', 'F0424/MID1', 'F0424/MID2', 'F0360/MID3', 'F0360/MID5', 'F0360/MID1', 'F0360/MID6', 'F0360/MID2', 'F0360/MID4', 'F0748/MID3', 'F0748/MID1', 'F0093/MID3', 'F0093/MID1', 'F0093/MID6', 'F0093/MID2', 'F0093/MID4', 'F0596/MID3', 'F0596/MID1', 'F0596/MID2', 'F0596/MID4', 'F0319/MID3', 'F0319/MID1', 'F0319/MID2', 'F0275/MID2', 'F0132/MID1', 'F0132/MID4', 'F0009/MID3', 'F0009/MID5', 'F0009/MID1', 'F0009/MID6', 'F0009/MID7', 'F0009/MID2', 'F0009/MID4', 'F0931/MID3', 'F0931/MID1', 'F0931/MID2', 'F0931/MID4', 'F0446/MID3', 'F0446/MID5', 'F0446/MID1', 'F0446/MID7', 'F0446/MID2', 'F0446/MID4', 'F0687/MID1', 'F0687/MID2', 'F0960/MID3', 'F0960/MID5', 'F0960/MID1', 'F0960/MID6', 'F0960/MID2', 'F0960/MID4', 'F0074/MID3', 'F0074/MID5', 'F0074/MID1', 'F0074/MID6', 'F0074/MID2', 'F0074/MID4', 'F0627/MID3', 'F0627/MID1', 'F0627/MID2', 'F0285/MID3', 'F0285/MID5', 'F0285/MID1', 'F0285/MID6', 'F0285/MID2', 'F0285/MID4', 'F0460/MID10', 'F0460/MID3', 'F0460/MID5', 'F0460/MID1', 'F0460/MID6', 'F0460/MID7', 'F0460/MID9', 'F0460/MID2', 'F0460/MID4', 'F0930/MID3', 'F0930/MID5', 'F0930/MID1', 'F0930/MID6', 'F0930/MID2', 'F0930/MID4', 'F0017/MID3', 'F0017/MID5', 'F0017/MID1', 'F0017/MID6', 'F0017/MID7', 'F0017/MID2', 'F0017/MID4', 'F0504/MID1', 'F0504/MID2', 'F0955/MID3', 'F0955/MID5', 'F0955/MID1', 'F0955/MID8', 'F0955/MID6', 'F0955/MID7', 'F0955/MID9', 'F0955/MID2', 'F0955/MID4', 'F0695/MID3', 'F0695/MID5', 'F0695/MID1', 'F0695/MID2', 'F0695/MID4', 'F0651/MID3', 'F0651/MID1', 'F0651/MID2', 'F0651/MID4', 'F0242/MID3', 'F0242/MID5', 'F0242/MID1', 'F0242/MID2', 'F0157/MID3', 'F0157/MID1', 'F0157/MID2', 'F0157/MID4', 'F0644/MID3', 'F0644/MID5', 'F0644/MID1', 'F0644/MID2', 'F0644/MID4', 'F0972/MID5', 'F0972/MID4', 'F0788/MID3', 'F0788/MID5', 'F0788/MID1', 'F0788/MID2', 'F0788/MID4', 'F0150/MID3', 'F0150/MID5', 'F0150/MID1', 'F0150/MID8', 'F0150/MID7', 'F0150/MID2', 'F0150/MID4', 'F0151/MID3', 'F0151/MID1', 'F0151/MID2', 'F0254/MID3', 'F0254/MID1', 'F0254/MID4', 'F0673/MID3', 'F0673/MID5', 'F0673/MID1', 'F0673/MID8', 'F0673/MID6', 'F0673/MID7', 'F0673/MID4', 'F0574/MID1', 'F0574/MID2', 'F0574/MID4', 'F0994/MID3', 'F0994/MID5', 'F0994/MID1', 'F0994/MID2', 'F0994/MID4', 'F0128/MID3', 'F0128/MID5', 'F0128/MID1', 'F0128/MID8', 'F0128/MID6', 'F0128/MID7', 'F0128/MID2', 'F0128/MID4', 'F0734/MID3', 'F0734/MID5', 'F0734/MID1', 'F0734/MID2', 'F0734/MID4', 'F0394/MID3', 'F0394/MID1', 'F0394/MID2', 'F0394/MID4', 'F0659/MID1', 'F0659/MID2', 'F0548/MID3', 'F0548/MID1', 'F0548/MID2', 'F0255/MID3', 'F0255/MID4', 'F0326/MID3', 'F0326/MID5', 'F0326/MID1', 'F0326/MID8', 'F0326/MID6', 'F0326/MID7', 'F0326/MID2', 'F0326/MID4', 'F0985/MID3', 'F0985/MID1', 'F0985/MID2', 'F0985/MID4', 'F0044/MID1', 'F0044/MID2', 'F0947/MID3', 'F0947/MID1', 'F0947/MID2', 'F0294/MID3', 'F0294/MID5', 'F0294/MID1', 'F0294/MID2', 'F0294/MID4', 'F0118/MID3', 'F0118/MID5', 'F0118/MID1', 'F0118/MID6', 'F0118/MID7', 'F0118/MID2', 'F0118/MID4', 'F0459/MID3', 'F0459/MID5', 'F0459/MID1', 'F0459/MID8', 'F0459/MID6', 'F0459/MID7', 'F0459/MID2', 'F0459/MID4', 'F0735/MID3', 'F0735/MID1', 'F0735/MID2', 'F0735/MID4', 'F0268/MID1', 'F0268/MID2', 'F0519/MID1', 'F0519/MID2', 'F0415/MID3', 'F0415/MID1', 'F0415/MID2', 'F0617/MID3', 'F0617/MID5', 'F0617/MID1', 'F0617/MID6', 'F0617/MID7', 'F0617/MID2', 'F0617/MID4', 'F0030/MID3', 'F0030/MID1', 'F0030/MID2', 'F0030/MID4', 'F0912/MID3', 'F0912/MID1', 'F0912/MID2', 'F0912/MID4', 'F0977/MID3', 'F0977/MID5', 'F0977/MID1', 'F0977/MID6', 'F0977/MID7', 'F0977/MID4', 'F0922/MID3', 'F0922/MID5', 'F0922/MID1', 'F0922/MID2', 'F0922/MID4', 'F0444/MID3', 'F0444/MID1', 'F0444/MID2', 'F0444/MID4', 'F0944/MID3', 'F0944/MID1', 'F0944/MID2', 'F0944/MID4', 'F0135/MID5', 'F0135/MID1', 'F0052/MID3', 'F0052/MID1', 'F0052/MID2', 'F0052/MID4', 'F0262/MID3', 'F0262/MID5', 'F0262/MID1', 'F0262/MID6', 'F0262/MID2', 'F0262/MID4', 'F0717/MID3', 'F0717/MID5', 'F0717/MID1', 'F0717/MID6', 'F0717/MID2', 'F0717/MID4', 'F0359/MID3', 'F0359/MID1', 'F0359/MID2', 'F0359/MID4', 'F0137/MID2', 'F0137/MID4', 'F0005/MID3', 'F0005/MID1', 'F0005/MID2', 'F0540/MID3', 'F0540/MID1', 'F0540/MID2', 'F0540/MID4', 'F0915/MID3', 'F0915/MID5', 'F0915/MID1', 'F0915/MID6', 'F0915/MID2', 'F0915/MID4', 'F0556/MID3', 'F0556/MID5', 'F0556/MID1', 'F0556/MID2', 'F0556/MID4', 'F0946/MID3', 'F0946/MID1', 'F0946/MID2', 'F0464/MID3', 'F0464/MID1', 'F0464/MID2', 'F0138/MID3', 'F0138/MID6', 'F0138/MID2', 'F0754/MID3', 'F0754/MID5', 'F0754/MID1', 'F0754/MID8', 'F0754/MID7', 'F0754/MID2', 'F0754/MID4', 'F0179/MID3', 'F0179/MID1', 'F0179/MID2', 'F0179/MID4', 'F0456/MID3', 'F0456/MID1', 'F0456/MID2', 'F0456/MID4', 'F0968/MID3', 'F0968/MID1', 'F0968/MID2', 'F0968/MID4', 'F0746/MID3', 'F0746/MID1', 'F0746/MID2', 'F0746/MID4', 'F0409/MID3', 'F0409/MID1', 'F0409/MID2', 'F0409/MID4', 'F0168/MID3', 'F0168/MID1', 'F0168/MID2', 'F0168/MID4', 'F0237/MID3', 'F0237/MID5', 'F0237/MID1', 'F0237/MID6', 'F0237/MID2', 'F0237/MID4', 'F0591/MID3', 'F0591/MID1', 'F0591/MID2', 'F0904/MID1', 'F0904/MID2', 'F0402/MID3', 'F0402/MID5', 'F0402/MID1', 'F0402/MID8', 'F0402/MID6', 'F0402/MID7', 'F0402/MID2', 'F0402/MID4', 'F0244/MID5', 'F0244/MID1', 'F0245/MID3', 'F0245/MID5', 'F0245/MID1', 'F0245/MID2', 'F0245/MID4', 'F0539/MID3', 'F0539/MID1', 'F0539/MID2', 'F0539/MID4', 'F0252/MID3', 'F0252/MID5', 'F0252/MID1', 'F0252/MID4', 'F0334/MID3', 'F0334/MID5', 'F0334/MID1', 'F0334/MID6', 'F0334/MID2', 'F0334/MID4', 'F0625/MID3', 'F0625/MID1', 'F0625/MID2', 'F0625/MID4', 'F0387/MID3', 'F0387/MID5', 'F0387/MID1', 'F0387/MID6', 'F0387/MID7', 'F0387/MID2', 'F0387/MID4', 'F0225/MID1', 'F0225/MID2', 'F0225/MID4', 'F0031/MID3', 'F0031/MID5', 'F0031/MID1', 'F0031/MID2', 'F0031/MID4', 'F0341/MID3', 'F0341/MID1', 'F0341/MID2', 'F0918/MID3', 'F0918/MID5', 'F0918/MID1', 'F0918/MID8', 'F0918/MID6', 'F0918/MID7', 'F0918/MID2', 'F0918/MID4', 'F0680/MID3', 'F0680/MID5', 'F0680/MID1', 'F0680/MID2', 'F0680/MID4', 'F0472/MID1', 'F0472/MID2', 'F0472/MID4', 'F0939/MID3', 'F0939/MID1', 'F0939/MID2', 'F0939/MID4', 'F0500/MID3', 'F0500/MID5', 'F0500/MID1', 'F0500/MID2', 'F0500/MID4', 'F0658/MID3', 'F0658/MID5', 'F0658/MID1', 'F0658/MID6', 'F0658/MID7', 'F0658/MID2', 'F0953/MID3', 'F0953/MID1', 'F0953/MID2', 'F0953/MID4', 'F0479/MID3', 'F0479/MID5', 'F0479/MID1', 'F0479/MID6', 'F0479/MID7', 'F0479/MID2', 'F0479/MID4', 'F0421/MID10', 'F0421/MID3', 'F0421/MID5', 'F0421/MID1', 'F0421/MID8', 'F0421/MID6', 'F0421/MID7', 'F0421/MID9', 'F0421/MID2', 'F0421/MID4', 'F0585/MID3', 'F0585/MID5', 'F0585/MID1', 'F0585/MID6', 'F0585/MID2', 'F0585/MID4', 'F0482/MID3', 'F0482/MID1', 'F0146/MID3', 'F0146/MID5', 'F0146/MID1', 'F0146/MID8', 'F0146/MID6', 'F0146/MID7', 'F0146/MID2', 'F0146/MID4', 'F0227/MID3', 'F0227/MID5', 'F0227/MID1', 'F0227/MID2', 'F0227/MID4', 'F0211/MID3', 'F0211/MID5', 'F0211/MID1', 'F0211/MID4', 'F0920/MID5', 'F0920/MID1', 'F0920/MID6', 'F0920/MID2', 'F0920/MID4', 'F0022/MID3', 'F0022/MID5', 'F0022/MID1', 'F0022/MID2', 'F0022/MID4', 'F0768/MID3', 'F0768/MID1', 'F0768/MID6', 'F0768/MID2', 'F0768/MID4', 'F0595/MID3', 'F0595/MID5', 'F0595/MID1', 'F0595/MID6', 'F0595/MID2', 'F0595/MID4', 'F0739/MID3', 'F0739/MID5', 'F0739/MID1', 'F0739/MID2', 'F0739/MID4', 'F0192/MID3', 'F0192/MID5', 'F0192/MID1', 'F0192/MID8', 'F0192/MID6', 'F0192/MID7', 'F0192/MID2', 'F0192/MID4', 'F0303/MID10', 'F0303/MID3', 'F0303/MID5', 'F0303/MID1', 'F0303/MID8', 'F0303/MID6', 'F0303/MID11', 'F0303/MID2', 'F0303/MID4', 'F0997/MID3', 'F0997/MID1', 'F0997/MID2', 'F0997/MID4', 'F0491/MID3', 'F0491/MID1', 'F0491/MID2', 'F0491/MID4', 'F0212/MID3', 'F0212/MID5', 'F0212/MID1', 'F0212/MID2', 'F0212/MID4', 'F0726/MID3', 'F0726/MID1', 'F0726/MID2', 'F0726/MID4', 'F0503/MID10', 'F0503/MID3', 'F0503/MID5', 'F0503/MID1', 'F0503/MID8', 'F0503/MID6', 'F0503/MID7', 'F0503/MID11', 'F0503/MID9', 'F0503/MID2', 'F0503/MID4', 'F0435/MID3', 'F0435/MID5', 'F0435/MID1', 'F0435/MID2', 'F0435/MID4', 'F0194/MID3', 'F0194/MID5', 'F0194/MID1', 'F0194/MID6', 'F0194/MID2', 'F0194/MID4', 'F0795/MID3', 'F0795/MID5', 'F0795/MID1', 'F0795/MID2', 'F0795/MID4', 'F0665/MID3', 'F0665/MID5', 'F0665/MID1', 'F0665/MID2', 'F0665/MID4', 'F0315/MID3', 'F0315/MID1', 'F0315/MID2', 'F0315/MID4', 'F0155/MID3', 'F0155/MID5', 'F0155/MID1', 'F0155/MID2', 'F0155/MID4', 'F0903/MID3', 'F0903/MID1', 'F0903/MID2', 'F0903/MID4', 'F0603/MID3', 'F0603/MID1', 'F0603/MID2', 'F0785/MID10', 'F0785/MID3', 'F0785/MID5', 'F0785/MID1', 'F0785/MID8', 'F0785/MID6', 'F0785/MID7', 'F0785/MID9', 'F0785/MID2', 'F0785/MID4', 'F0526/MID3', 'F0526/MID5', 'F0526/MID1', 'F0526/MID2', 'F0526/MID4', 'F0465/MID1', 'F0465/MID2', 'F0465/MID4', 'F0536/MID3', 'F0536/MID5', 'F0536/MID1', 'F0536/MID6', 'F0536/MID2', 'F0536/MID4', 'F0298/MID3', 'F0298/MID5', 'F0298/MID1', 'F0298/MID6', 'F0298/MID2', 'F0298/MID4', 'F0258/MID3', 'F0258/MID1', 'F0258/MID4', 'F0914/MID3', 'F0914/MID1', 'F0914/MID2', 'F0914/MID4', 'F0020/MID10', 'F0020/MID3', 'F0020/MID5', 'F0020/MID1', 'F0020/MID8', 'F0020/MID6', 'F0020/MID7', 'F0020/MID11', 'F0020/MID9', 'F0020/MID2', 'F0336/MID3', 'F0336/MID1', 'F0322/MID1', 'F0322/MID2', 'F0178/MID3', 'F0178/MID1', 'F0178/MID2', 'F0728/MID3', 'F0728/MID5', 'F0728/MID1', 'F0728/MID2', 'F0728/MID4', 'F0378/MID3', 'F0378/MID5', 'F0378/MID1', 'F0378/MID6', 'F0378/MID2', 'F0378/MID4', 'F0770/MID3', 'F0770/MID5', 'F0770/MID1', 'F0770/MID2', 'F0952/MID3', 'F0952/MID1', 'F0952/MID2', 'F0952/MID4', 'F0248/MID3', 'F0248/MID1', 'F0248/MID2', 'F0425/MID10', 'F0425/MID3', 'F0425/MID5', 'F0425/MID1', 'F0425/MID8', 'F0425/MID6', 'F0425/MID7', 'F0425/MID9', 'F0425/MID2', 'F0425/MID4', 'F0339/MID3', 'F0339/MID5', 'F0339/MID1', 'F0339/MID2', 'F0339/MID4', 'F0086/MID3', 'F0086/MID1', 'F0086/MID2', 'F0086/MID4', 'F0423/MID3', 'F0423/MID5', 'F0423/MID1', 'F0423/MID7', 'F0423/MID2', 'F0423/MID4', 'F0160/MID3', 'F0160/MID5', 'F0160/MID6', 'F0160/MID7', 'F0160/MID4', 'F0545/MID3', 'F0545/MID5', 'F0545/MID1', 'F0545/MID6', 'F0545/MID2', 'F0545/MID4', 'F0321/MID3', 'F0321/MID1', 'F0321/MID2', 'F0969/MID3', 'F0969/MID5', 'F0969/MID1', 'F0969/MID2', 'F0365/MID3', 'F0365/MID1', 'F0365/MID2', 'F0725/MID3', 'F0725/MID1', 'F0725/MID2', 'F0733/MID3', 'F0733/MID5', 'F0733/MID1', 'F0733/MID2', 'F0733/MID4', 'F0773/MID3', 'F0773/MID5', 'F0773/MID1', 'F0773/MID6', 'F0773/MID2', 'F0773/MID4', 'F0426/MID3', 'F0426/MID5', 'F0426/MID1', 'F0426/MID4', 'F0266/MID1', 'F0266/MID2', 'F0760/MID3', 'F0760/MID1', 'F0760/MID2', 'F0760/MID4', 'F0309/MID3', 'F0309/MID5', 'F0309/MID1', 'F0309/MID6', 'F0309/MID2', 'F0532/MID3', 'F0532/MID5', 'F0532/MID1', 'F0532/MID4', 'F0271/MID3', 'F0271/MID2', 'F0497/MID3', 'F0497/MID1', 'F0497/MID2', 'F0241/MID3', 'F0241/MID1', 'F0241/MID2', 'F0105/MID3', 'F0105/MID5', 'F0105/MID1', 'F0105/MID6', 'F0105/MID2', 'F0105/MID4', 'F0101/MID10', 'F0101/MID3', 'F0101/MID14', 'F0101/MID5', 'F0101/MID1', 'F0101/MID8', 'F0101/MID6', 'F0101/MID7', 'F0101/MID15', 'F0101/MID9', 'F0101/MID2', 'F0101/MID13', 'F0101/MID4', 'F0101/MID12', 'F0041/MID1', 'F0041/MID6', 'F0041/MID4', 'F0126/MID3', 'F0126/MID5', 'F0126/MID1', 'F0126/MID6', 'F0126/MID2', 'F0126/MID4', 'F0552/MID10', 'F0552/MID3', 'F0552/MID5', 'F0552/MID1', 'F0552/MID8', 'F0552/MID6', 'F0552/MID7', 'F0552/MID11', 'F0552/MID9', 'F0552/MID2', 'F0552/MID13', 'F0552/MID4', 'F0552/MID12', 'F0908/MID3', 'F0908/MID1', 'F0908/MID2', 'F0908/MID4', 'F0742/MID3', 'F0742/MID5', 'F0742/MID1', 'F0742/MID8', 'F0742/MID6', 'F0742/MID7', 'F0742/MID2', 'F0742/MID4', 'F0209/MID3', 'F0209/MID5', 'F0209/MID1', 'F0209/MID6', 'F0209/MID7', 'F0209/MID2', 'F0209/MID4', 'F0129/MID3', 'F0129/MID5', 'F0129/MID2', 'F0129/MID4', 'F0533/MID3', 'F0533/MID5', 'F0533/MID1', 'F0533/MID6', 'F0533/MID2', 'F0533/MID4', 'F0933/MID3', 'F0933/MID5', 'F0933/MID1', 'F0933/MID2', 'F0933/MID4', 'F0708/MID10', 'F0708/MID3', 'F0708/MID5', 'F0708/MID1', 'F0708/MID8', 'F0708/MID6', 'F0708/MID7', 'F0708/MID9', 'F0708/MID2', 'F0599/MID3', 'F0599/MID5', 'F0599/MID1', 'F0599/MID6', 'F0599/MID2', 'F0599/MID4', 'F0427/MID3', 'F0427/MID5', 'F0427/MID1', 'F0427/MID6', 'F0427/MID2', 'F0427/MID4', 'F0724/MID3', 'F0724/MID1', 'F0724/MID2', 'F0283/MID3', 'F0283/MID5', 'F0283/MID1', 'F0283/MID2', 'F0283/MID4', 'F0516/MID3', 'F0516/MID1', 'F0516/MID2', 'F0516/MID4', 'F0646/MID3', 'F0646/MID5', 'F0646/MID1', 'F0646/MID6', 'F0646/MID2', 'F0646/MID4', 'F0970/MID3', 'F0970/MID1', 'F0970/MID2', 'F0210/MID3', 'F0210/MID1', 'F0210/MID2', 'F0210/MID4', 'F0481/MID3', 'F0481/MID5', 'F0481/MID1', 'F0481/MID8', 'F0481/MID6', 'F0481/MID9', 'F0481/MID2', 'F0481/MID4', 'F0982/MID3', 'F0982/MID1', 'F0982/MID2', 'F0621/MID3', 'F0621/MID5', 'F0621/MID1', 'F0621/MID2', 'F0621/MID4', 'F0761/MID3', 'F0761/MID1', 'F0761/MID2', 'F0716/MID3', 'F0716/MID5', 'F0716/MID1', 'F0716/MID2', 'F0716/MID4', 'F0068/MID3', 'F0068/MID5', 'F0068/MID1', 'F0068/MID2', 'F0068/MID4', 'F0431/MID2', 'F0431/MID4', 'F0721/MID5', 'F0721/MID1', 'F0721/MID6', 'F0721/MID7', 'F0721/MID2', 'F0721/MID4', 'F0543/MID3', 'F0543/MID5', 'F0543/MID1', 'F0543/MID2', 'F0543/MID4', 'F0763/MID3', 'F0763/MID5', 'F0763/MID1', 'F0763/MID8', 'F0763/MID6', 'F0763/MID7', 'F0763/MID9', 'F0763/MID2', 'F0763/MID4', 'F0216/MID3', 'F0216/MID5', 'F0216/MID1', 'F0216/MID6', 'F0216/MID7', 'F0216/MID2', 'F0216/MID4', 'F0257/MID3', 'F0257/MID5', 'F0257/MID1', 'F0257/MID2', 'F0257/MID4', 'F0778/MID3', 'F0778/MID1', 'F0778/MID4', 'F0287/MID3', 'F0287/MID1', 'F0287/MID2', 'F0287/MID4', 'F0714/MID3', 'F0714/MID1', 'F0714/MID2', 'F0714/MID4', 'F0198/MID3', 'F0198/MID5', 'F0198/MID1', 'F0198/MID2', 'F0198/MID4', 'F0002/MID3', 'F0002/MID1', 'F0002/MID2', 'F0236/MID3', 'F0236/MID1', 'F0236/MID2', 'F0236/MID4', 'F0411/MID3', 'F0411/MID1', 'F0411/MID2', 'F0142/MID3', 'F0142/MID5', 'F0142/MID8', 'F0142/MID6', 'F0281/MID3', 'F0281/MID1', 'F0281/MID2', 'F0281/MID4', 'F0363/MID3', 'F0363/MID5', 'F0363/MID1', 'F0363/MID8', 'F0363/MID6', 'F0363/MID7', 'F0363/MID2', 'F0974/MID3', 'F0974/MID5', 'F0974/MID1', 'F0974/MID6', 'F0974/MID7', 'F0974/MID2', 'F0350/MID3', 'F0350/MID5', 'F0350/MID1', 'F0350/MID2', 'F0350/MID4', 'F0174/MID3', 'F0174/MID5', 'F0174/MID1', 'F0174/MID6', 'F0174/MID7', 'F0174/MID2', 'F0174/MID4', 'F0195/MID3', 'F0195/MID1', 'F0195/MID2', 'F1000/MID3', 'F1000/MID5', 'F1000/MID1', 'F1000/MID8', 'F1000/MID6', 'F1000/MID7', 'F1000/MID9', 'F1000/MID2', 'F1000/MID4', 'F0231/MID10', 'F0231/MID3', 'F0231/MID14', 'F0231/MID5', 'F0231/MID1', 'F0231/MID8', 'F0231/MID6', 'F0231/MID7', 'F0231/MID11', 'F0231/MID9', 'F0231/MID4', 'F0231/MID12', 'F0123/MID3', 'F0123/MID5', 'F0123/MID1', 'F0123/MID6', 'F0123/MID7', 'F0123/MID2', 'F0123/MID4', 'F0756/MID3', 'F0756/MID5', 'F0756/MID1', 'F0756/MID2', 'F0756/MID4', 'F0036/MID3', 'F0036/MID5', 'F0036/MID1', 'F0036/MID6', 'F0036/MID7', 'F0036/MID2', 'F0036/MID4', 'F0951/MID3', 'F0951/MID1', 'F0951/MID2'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iveCZleFyqej",
        "colab_type": "code",
        "outputId": "7163a394-2a3f-4290-8472-51fa84035e1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_person_to_images_map.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2051"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mweDgnOJy4bJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_person_to_images_map = defaultdict(list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK_LgUg0zBeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for x in val_images:\n",
        "    val_person_to_images_map[x.split(\"/\")[-3] + \"/\" + x.split(\"/\")[-2]].append(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMENkp89zFhY",
        "colab_type": "code",
        "outputId": "25b887ea-d98f-499f-d1d0-12786b827e96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(val_person_to_images_map )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "265"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MemVR1Ga7_DH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p=set(ppl)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtPwP2rG7_Bz",
        "colab_type": "code",
        "outputId": "b8876731-f11a-429b-9d74-345b1a4c7627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(p)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2316"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmReeHPkzLcy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "relationships = pd.read_csv(train_file_path)\n",
        "relationships = list(zip(relationships.p1.values, relationships.p2.values))\n",
        "relationships = [x for x in relationships if x[0] in ppl and x[1] in ppl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwpuA9SqzONX",
        "colab_type": "code",
        "outputId": "d53adc0c-800a-4a82-e051-8de51f0922e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "relationships[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('F0002/MID2', 'F0002/MID3')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNo1dBsM8zkE",
        "colab_type": "code",
        "outputId": "35852442-0a07-44cc-9b34-43e2ea93d124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(relationships)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3362"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34nsdNlv2GLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = [x for x in relationships if val_famillies not in x[0]]\n",
        "val = [x for x in relationships if val_famillies in x[0]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_40wag92NOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_img(path):\n",
        "    img = cv2.imread(path)\n",
        "    img = np.array(img).astype(np.float)\n",
        "    return preprocess_input(img, version=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HQMbwX_2Wnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
        "    ppl = list(person_to_images_map.keys())\n",
        "    while True:\n",
        "        batch_tuples = sample(list_tuples, batch_size // 2)\n",
        "        labels = [1] * len(batch_tuples)\n",
        "        while len(batch_tuples) < batch_size:\n",
        "            p1 = choice(ppl)\n",
        "            p2 = choice(ppl)\n",
        "\n",
        "            if p1 != p2 and (p1, p2) not in list_tuples and (p2, p1) not in list_tuples:\n",
        "                batch_tuples.append((p1, p2))\n",
        "                labels.append(0)\n",
        "\n",
        "        for x in batch_tuples:\n",
        "            if not len(person_to_images_map[x[0]]):\n",
        "                print(x[0])\n",
        "\n",
        "        X1 = [choice(person_to_images_map[x[0]]) for x in batch_tuples]\n",
        "        X1 = np.array([read_img(x) for x in X1])\n",
        "\n",
        "        X2 = [choice(person_to_images_map[x[1]]) for x in batch_tuples]\n",
        "        X2 = np.array([read_img(x) for x in X2])\n",
        "\n",
        "        yield [X1, X2], labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxogtVKGNZfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHsNK1xY2hNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def baseline_model():\n",
        "    input_1 = Input(shape=(224, 224, 3))\n",
        "    input_2 = Input(shape=(224, 224, 3))\n",
        "\n",
        "    base_model = VGGFace(model='resnet50', include_top=False)\n",
        "\n",
        "    for x in base_model.layers[:]:\n",
        "        x.trainable = True\n",
        "\n",
        "    x1 = base_model(input_1)\n",
        "    x2 = base_model(input_2)\n",
        "\n",
        "    # x1_ = Reshape(target_shape=(7*7, 2048))(x1)\n",
        "    # x2_ = Reshape(target_shape=(7*7, 2048))(x2)\n",
        "    #\n",
        "    # x_dot = Dot(axes=[2, 2], normalize=True)([x1_, x2_])\n",
        "    # x_dot = Flatten()(x_dot)\n",
        "\n",
        "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
        "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
        "\n",
        "    x3 = Subtract()([x1, x2])\n",
        "    x3 = Multiply()([x3, x3])\n",
        "\n",
        "    x = Multiply()([x1, x2])\n",
        "\n",
        "    x = Concatenate(axis=-1)([x, x3])\n",
        "\n",
        "    x = Dense(100, activation=\"relu\")(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    out = Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "    model = Model([input_1, input_2], out)\n",
        "\n",
        "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN9WTue42yW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_path = \"vgg_face_weight.h5\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgRsOacQ29_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNino173VOY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpGoHfBG3H9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_loss\", mode=\"min\", factor=0.1, patience=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VckKj4Uj3PEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks_list = [checkpoint, reduce_on_plateau]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vjy6QqeH3UpJ",
        "colab_type": "code",
        "outputId": "67569341-b110-409c-aa76-6cf4761a1ccb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "model = baseline_model()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_7 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_8 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "vggface_resnet50 (Model)        multiple             23561152    input_7[0][0]                    \n",
            "                                                                 input_8[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_5 (GlobalM (None, 2048)         0           vggface_resnet50[1][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_5 (Glo (None, 2048)         0           vggface_resnet50[1][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling2d_6 (GlobalM (None, 2048)         0           vggface_resnet50[2][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_6 (Glo (None, 2048)         0           vggface_resnet50[2][0]           \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_7 (Concatenate)     (None, 4096)         0           global_max_pooling2d_5[0][0]     \n",
            "                                                                 global_average_pooling2d_5[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 4096)         0           global_max_pooling2d_6[0][0]     \n",
            "                                                                 global_average_pooling2d_6[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "subtract_3 (Subtract)           (None, 4096)         0           concatenate_7[0][0]              \n",
            "                                                                 concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_6 (Multiply)           (None, 4096)         0           concatenate_7[0][0]              \n",
            "                                                                 concatenate_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "multiply_5 (Multiply)           (None, 4096)         0           subtract_3[0][0]                 \n",
            "                                                                 subtract_3[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_9 (Concatenate)     (None, 8192)         0           multiply_6[0][0]                 \n",
            "                                                                 multiply_5[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 100)          819300      concatenate_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 100)          0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_6 (Dense)                 (None, 1)            101         dropout_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 24,380,553\n",
            "Trainable params: 24,327,433\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awBqCwhy3s_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlrBD-6p3cMA",
        "colab_type": "code",
        "outputId": "7e108170-0fa7-483c-a024-16901e08e228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "model.fit_generator(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=True,\n",
        "                    validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=25, verbose=2,\n",
        "                    workers=4, callbacks=callbacks_list, steps_per_epoch=200, validation_steps=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:44: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py:272: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " - 207s - loss: 3.3803 - acc: 0.5447 - val_loss: 3.8153 - val_acc: 0.5319\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 3.81527, saving model to vgg_face_weight.h5\n",
            "Epoch 2/25\n",
            " - 186s - loss: 1.9717 - acc: 0.5969 - val_loss: 2.2034 - val_acc: 0.5825\n",
            "\n",
            "Epoch 00002: val_loss improved from 3.81527 to 2.20337, saving model to vgg_face_weight.h5\n",
            "Epoch 3/25\n",
            " - 187s - loss: 1.0793 - acc: 0.6272 - val_loss: 1.3290 - val_acc: 0.6000\n",
            "\n",
            "Epoch 00003: val_loss improved from 2.20337 to 1.32901, saving model to vgg_face_weight.h5\n",
            "Epoch 4/25\n",
            " - 187s - loss: 0.7764 - acc: 0.6544 - val_loss: 0.8824 - val_acc: 0.6275\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.32901 to 0.88238, saving model to vgg_face_weight.h5\n",
            "Epoch 5/25\n",
            " - 188s - loss: 0.5877 - acc: 0.7006 - val_loss: 0.7714 - val_acc: 0.6444\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.88238 to 0.77136, saving model to vgg_face_weight.h5\n",
            "Epoch 6/25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGP06Yi1TCC9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_path = \"../content/test/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYDGN0SQQOoe",
        "colab_type": "code",
        "outputId": "542f1fcf-ed58-40b0-f032-9ff0f2d08826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNwLnnfOaA6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def chunker(seq, size=32):\n",
        "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGprNqdpaEoS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "submission = pd.read_csv('../content/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRPGzHZBaPSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03aH73WAaT_4",
        "colab_type": "code",
        "outputId": "48866433-422b-4eb5-b3c7-a3dc71292411",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for batch in tqdm(chunker(submission.img_pair.values)):\n",
        "    X1 = [x.split(\"-\")[0] for x in batch]\n",
        "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
        "\n",
        "    X2 = [x.split(\"-\")[1] for x in batch]\n",
        "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
        "\n",
        "    pred = model.predict([X1, X2]).ravel().tolist()\n",
        "    predictions += pred\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "166it [01:07,  1.14it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIOKPC6IaZXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission['is_related'] = predictions\n",
        "\n",
        "submission.to_csv(\"vgg_face_f.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av4qoriiavhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}